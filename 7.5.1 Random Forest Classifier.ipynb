{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and imports.\n",
    "\n",
    "BASE_NUM = 1\n",
    "RANDOM_STATE = None\n",
    "CV = 5\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "\n",
    "data = pd.read_csv(os.path.join('datasets', 'base_{}.csv'.format(BASE_NUM)), sep=';')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train / test\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "for train_index, test_index in split.split(data, data['DROPPED_OUT']):\n",
    "    train_set = data.loc[train_index]\n",
    "    test_set = data.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the training set has an equal split of students that dropped out and graduated.\n",
    "\n",
    "train_set['DROPPED_OUT'].value_counts() / len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the testing set has an equal split of students that dropped out and graduated.\n",
    "\n",
    "test_set['DROPPED_OUT'].value_counts() / len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the training set to 'data' for convenience.\n",
    "\n",
    "data = train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_set.drop(\"DROPPED_OUT\", axis=1) # drop labels for training set\n",
    "data_labels = train_set[\"DROPPED_OUT\"].copy()\n",
    "\n",
    "test_data = test_set.drop(\"DROPPED_OUT\", axis=1) # drop labels for testing set\n",
    "test_labels = test_set[\"DROPPED_OUT\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for standard scaling and translating categories to numbers.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, Imputer\n",
    "\n",
    "from msc_preprocessing import CourseOfStudyNamer, CategoricalEncoder, DataFrameSelector\n",
    "from msc_preprocessing import ElementaryNameFixer, ElementarySchoolDistance\n",
    "from msc_preprocessing import NationalitySelector\n",
    "\n",
    "cat_attribs = ['COURSE_OF_STUDY', 'SCHOOL', 'NATIONALITY', ]\n",
    "\n",
    "num_attribs = list(data.drop(cat_attribs + ['ELEMENTARY_SCHOOL'], axis=1)) + ['ELEMENTARY_SCHOOL_DISTANCE']\n",
    "\n",
    "# A pipeline for numerical attributes.\n",
    "num_pipeline = Pipeline([\n",
    "        ('elementary_school_fix_names', ElementaryNameFixer()),\n",
    "        ('elementary_school_distance', ElementarySchoolDistance()),\n",
    "        ('selector', DataFrameSelector(num_attribs)), # Select only data that has numbers.\n",
    "        ('imputer', Imputer(strategy=\"median\")), # Replace NULL values with averages.\n",
    "        ('std_scaler', RobustScaler()), # Scale all numerical values to the same scale.\n",
    "    ])\n",
    "\n",
    "# A pipeline for categorial attributes.\n",
    "cat_pipeline = Pipeline([\n",
    "        ('course_of_study_fix_names', CourseOfStudyNamer()),\n",
    "        ('nationality_selector', NationalitySelector()),\n",
    "        ('selector', DataFrameSelector(cat_attribs)), # Select only data that has categories.\n",
    "        ('cat_encoder', CategoricalEncoder(encoding=\"onehot-dense\", handle_unknown='ignore')), # Translate categories to numbers.\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Merge the numerical and categorical pipelines.\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the data for training.\n",
    "data_prepared = full_pipeline.fit_transform(data)\n",
    "data_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# RandomForestClassifier Grid Search.\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 15, 30, 50, 150, 200, 400, 600, 800, 1000, 1500],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False],\n",
    "}\n",
    "\n",
    "cls = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "grid_search = GridSearchCV(cls, \n",
    "                           param_grid, \n",
    "                           cv=CV,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=2,\n",
    "                           scoring='f1')\n",
    "grid_search.fit(data_prepared, data_labels)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "cls = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Grid Search.\n",
    "\n",
    "import itertools\n",
    "\n",
    "cv_results = grid_search.cv_results_\n",
    "\n",
    "# Parameters and names.\n",
    "grid_param_1 = param_grid['n_estimators']\n",
    "grid_param_2 = param_grid['max_features']\n",
    "grid_param_3 = param_grid['bootstrap']\n",
    "name_param_1 = 'N Estimators'\n",
    "name_param_2 = 'Max Features'\n",
    "name_param_3 = 'Bootstrap'\n",
    "\n",
    "# Get f1 scores for each grid search\n",
    "scores_mean = cv_results['mean_test_score']\n",
    "scores_mean = np.array(scores_mean).reshape(len(grid_param_2) * len(grid_param_3), len(grid_param_1))\n",
    "\n",
    "# Plot scores\n",
    "plt.rcParams['figure.facecolor'] = '#FFFFFF'\n",
    "_, ax = plt.subplots(1,1, figsize=(15, 15))\n",
    "\n",
    "# Param1 is the X-axis, Param 2 and 3 are the Y-axis.\n",
    "for idx, val in enumerate(itertools.product(grid_param_3, grid_param_2)):\n",
    "    #if (\n",
    "    #   False or\n",
    "    #   (val[0] == True  and val[1] is None) or\n",
    "    #   (val[0] == False and val[1] is None) or\n",
    "    #   (val[0] == True  and val[1] == 'sqrt') or\n",
    "    #   (val[0] == False and val[1] == 'sqrt') or\n",
    "    #   (val[0] == True and val[1] == 'log2') or\n",
    "    #   (val[0] == False and val[1] == 'log2') or\n",
    "    #   False\n",
    "    #):\n",
    "    ax.plot(\n",
    "        grid_param_1, \n",
    "        scores_mean[idx,:], \n",
    "        '-o', \n",
    "        label='{}: {}, {}: {}'.format(name_param_3, val[0], name_param_2, val[1])\n",
    "    )\n",
    "\n",
    "# Format plot\n",
    "ax.set_title('RandomForestClassifier')\n",
    "ax.set_xlabel(name_param_1)\n",
    "ax.set_ylabel('CV Average Score')\n",
    "ax.legend(loc='best', fontsize=12)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the scores.\n",
    "\n",
    "for mean_score, params in zip(cv_results[\"mean_test_score\"], cv_results[\"params\"]):\n",
    "    print(mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print feature importances.\n",
    "\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "extra_attribs = [] # Not needed now. Keep for future possibilities.\n",
    "cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross-validation with k=CV\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Binary classifier. Classifies as true / false.\n",
    "y_train_pred_binary = cross_val_predict(cls, data_prepared, data_labels, cv=CV, method='predict')\n",
    "\n",
    "y_train_pred_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# Performance of binary classifier\n",
    "#\n",
    "#\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(data_labels, y_train_pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ac', accuracy_score(data_labels, y_train_pred_binary))\n",
    "print('f1', f1_score(data_labels, y_train_pred_binary))\n",
    "print('pr', precision_score(data_labels, y_train_pred_binary))\n",
    "print('re', recall_score(data_labels, y_train_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Run test data (x% split).\n",
    "#\n",
    "\n",
    "test_data_prepared = full_pipeline.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# Performance of binary on test data (x% split).\n",
    "#\n",
    "#\n",
    "\n",
    "final_predictions = cls.predict(test_data_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_labels, final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ac', accuracy_score(test_labels, final_predictions))\n",
    "print('f1', f1_score(test_labels, final_predictions))\n",
    "print('pr', precision_score(test_labels, final_predictions))\n",
    "print('re', recall_score(test_labels, final_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
